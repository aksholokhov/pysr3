{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import threading\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipyparallel import Client \n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.oracles import LinearLMEOracle, LinearLMEOracleRegularized\n",
    "from lib.solvers import LinearLMESolver, LinearLMERegSolver\n",
    "from lib.problems import LinearLMEProblem\n",
    "from lib.feature_selection import FeatureSelectorV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LME Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=212, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=10, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=10, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-6, max=9, value=1e-5, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-10, max=9, value=0.1, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='EM',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1,\n",
    "                 lb=0.1,\n",
    "                 lg=1,\n",
    "                 obs_std=0.1, \n",
    "                 bootstrap='Nonparametric',\n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='EM',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[0] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "        \n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "\n",
    "        \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        #gamma_reg_exact_full, g_opt = train_oracle.good_lambda_gamma(mode=\"exact_full_hess\")\n",
    "        #gamma_reg_exact_full = lg\n",
    "        #train_oracle.lg = gamma_reg_exact_full\n",
    "        #test_oracle.lg = gamma_reg_exact_full\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "        logger = model.fit(train_oracle, test_oracle, method=method, initializer=initializer)\n",
    "        #print(logger[\"grad_gamma_norm\"])\n",
    "    else:    \n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        logger = model.fit(train_oracle, test_oracle, method=method, initializer=initializer)\n",
    "    \n",
    "    # Eigenvalues and eigenvectors at the solution\n",
    "    #print(train_oracle.gradient_gamma(beta, empirical_gamma))\n",
    "    #print(np.linalg.eigvals(train_oracle.hessian_gamma(beta, empirical_gamma)))\n",
    "    \n",
    "    if not logger[\"converged\"]:\n",
    "        print(\"Did not converge\") \n",
    "        \n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "    prediction_plot = fig.add_subplot(grid[:2, :2])\n",
    "\n",
    "\n",
    "    model_parameters_plot = fig.add_subplot(grid[0, 2])\n",
    "    pred_beta = model.beta\n",
    "    model_parameters_plot.scatter(true_parameters['beta'], pred_beta)\n",
    "    model_parameters_plot.set_xlabel(\"True parameters\")\n",
    "    model_parameters_plot.set_ylabel(\"Inferred parameters\")\n",
    "    model_parameters_low_lim = -0.5#min(min(pred_beta), min(true_parameters['beta'])) - 0.1\n",
    "    model_parameters_high_lim = 1.5#max(max(pred_beta), max(true_parameters['beta'])) + 0.1\n",
    "    model_parameters_plot.set_xlim(model_parameters_low_lim, model_parameters_high_lim)\n",
    "    model_parameters_plot.set_ylim(model_parameters_low_lim, model_parameters_high_lim)\n",
    "    model_parameters_plot.plot(model_parameters_plot.get_xlim(), model_parameters_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    \n",
    "    \n",
    "    random_effects_plot = fig.add_subplot(grid[1, 2])\n",
    "    random_effects_low_lim = 0\n",
    "    random_effects_high_lim = 0\n",
    "    for i, (u_pred, u_true) in enumerate(zip(model.us, true_parameters['random_effects'])):\n",
    "        random_effects_plot.scatter(u_true, u_pred, label=\"Study %d\" % (i + 1), c=color_map[i])\n",
    "        random_effects_low_lim = min(min(u_true), min(u_pred), random_effects_low_lim)\n",
    "        random_effects_high_lim = max(max(u_true), max(u_pred), random_effects_high_lim)\n",
    "    random_effects_plot.legend()\n",
    "    random_effects_low_lim -= 0.1\n",
    "    random_effects_high_lim += 0.1\n",
    "    random_effects_plot.set_xlim(random_effects_low_lim, random_effects_high_lim)\n",
    "    random_effects_plot.set_ylim(random_effects_low_lim, random_effects_high_lim)\n",
    "    random_effects_plot.plot(random_effects_plot.get_xlim(), random_effects_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    random_effects_plot.set_xlabel(\"True random effects\")\n",
    "    random_effects_plot.set_ylabel(\"Inferred random effects\")\n",
    "\n",
    "    gamma_to_show = model.gamma\n",
    "    gamma_plot = fig.add_subplot(grid[0, 3])\n",
    "    gamma_plot.scatter(empirical_gamma, gamma_to_show, label=\"Inferred\")\n",
    "    gamma_low_lim = min(min(empirical_gamma), min(gamma_to_show)) - 0.2\n",
    "    gamma_high_lim = max(max(empirical_gamma), max(gamma_to_show)) + 0.2\n",
    "    gamma_plot.set_xlim(gamma_low_lim, gamma_high_lim)\n",
    "    gamma_plot.set_ylim(gamma_low_lim, gamma_high_lim)\n",
    "    gamma_plot.plot(gamma_plot.get_xlim(), gamma_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    gamma_plot.set_xlabel(\"Empirical gamma\")\n",
    "    gamma_plot.set_ylabel(\"Inferred gamma\")\n",
    "\n",
    "    abs_min = np.inf\n",
    "    if num_random_effects == 2:\n",
    "        trajectory_plot = fig.add_subplot(grid[2:4, 0:2])\n",
    "    \n",
    "        xlims = [0, 2] #ax.get_xlim()\n",
    "        ylims = [0, 2] #ax.get_ylim()\n",
    "\n",
    "        plot_resolution = 100\n",
    "        x = np.linspace(0, xlims[1], plot_resolution)\n",
    "        y = np.linspace(0, ylims[1], plot_resolution)\n",
    "        z = np.zeros((plot_resolution, plot_resolution))\n",
    "        zh = np.zeros((plot_resolution, plot_resolution))\n",
    "        def psd(hessian):\n",
    "            eigvals = np.linalg.eigvals(hessian)\n",
    "            if np.linalg.norm(np.imag(eigvals)) > 1e-15:\n",
    "                return -1\n",
    "            min_eigval = min(np.real(eigvals))\n",
    "            if min_eigval < 0:\n",
    "                return -1\n",
    "            else:\n",
    "                return min_eigval\n",
    "        for i, g2 in enumerate(y):\n",
    "            for j, g1 in enumerate(x):\n",
    "                gamma0 = np.array([g1, g2])\n",
    "                beta0 = train_oracle.optimal_beta(gamma0)\n",
    "                z[i, j] = train_oracle.loss(beta0, gamma0)\n",
    "                hessian = train_oracle.hessian_gamma(beta0, gamma0)\n",
    "                zh[i, j] = psd(hessian)\n",
    "\n",
    "        abs_min = min(min(logger[\"loss\"]), np.min(z)) - 1e-8\n",
    "\n",
    "        csh = trajectory_plot.contourf(x, y, zh, levels=[0, 1e13], colors=\"lightgreen\")\n",
    "\n",
    "        levels = np.min(z) + np.array([1e-2, 1e-1, 1e0, 1e1, 1e2])\n",
    "        levels_labels = [\"1e-2\", \"1e-1\", \"1e0\", \"1e1\", \"1e2\"]\n",
    "        levels_dict = {levels[i]: levels_labels[i] for i in range(len(levels_labels))}\n",
    "\n",
    "        cs = trajectory_plot.contour(x, y, z, levels=levels)\n",
    "        plt.clabel(cs, fontsize=8, fmt=levels_dict)\n",
    "\n",
    "\n",
    "        gamma_trace = np.array(logger[\"gamma\"]).T\n",
    "        trajectory_plot.plot(gamma_trace[0], gamma_trace[1], '-ob', label = method)\n",
    "\n",
    "        trajectory_plot.set_xlim(xlims)\n",
    "        trajectory_plot.set_ylim(ylims)\n",
    "        trajectory_plot.set_xlabel(r\"$\\gamma_1$, variation of the first random effect\")\n",
    "        trajectory_plot.set_ylabel(r\"$\\gamma_2$, variation of the second random effect\")\n",
    "        \n",
    "        extra_plot = fig.add_subplot(grid[2:4, 2:4])\n",
    "        \n",
    "        def psd_criterion(gamma_t):\n",
    "            beta_t = train_oracle.optimal_beta(gamma_t)\n",
    "            criterion_satisfied = []\n",
    "            gamma_mat = np.diag(gamma_t)\n",
    "            for x, y, z, l in train:\n",
    "                omega = z.dot(gamma_mat).dot(z.T) + l\n",
    "                xi = y - x.dot(beta_t)\n",
    "                criterion_satisfied.append(xi.T.dot(np.linalg.inv(omega)).dot(xi) <= 1/2)\n",
    "            return sum(criterion_satisfied)\n",
    "        \n",
    "        zc = np.zeros((plot_resolution, plot_resolution))\n",
    "        for i, g2 in enumerate(y):\n",
    "            for j, g1 in enumerate(x):\n",
    "                gamma_t = np.array([g1, g2])\n",
    "                zc[i, j] = psd_criterion(gamma_t)\n",
    "                                \n",
    "        cc = extra_plot.contourf(x, y, zc)\n",
    "        cs = extra_plot.contour(x, y, z, levels=levels)\n",
    "        plt.clabel(cs, fontsize=8, fmt=levels_dict)\n",
    "        extra_plot.set_xlim(xlims)\n",
    "        extra_plot.set_ylim(ylims)\n",
    "        extra_plot.set_xlabel(r\"$\\gamma_1$, variation of the first random effect\")\n",
    "        extra_plot.set_ylabel(r\"$\\gamma_2$, variation of the second random effect\")\n",
    "\n",
    "    #loss_plot = fig.add_subplot(grid[1, 3])\n",
    "    #min_loss = min(np.min(logger[\"loss\"]), abs_min)\n",
    "    #loss_plot.semilogy(range(len(logger[\"loss\"])), logger[\"loss\"] - min_loss + 1e-8, '-ob', label=method)\n",
    "    #loss_plot.set_ylim((1e-8, 1e3))\n",
    "    \n",
    "    grad_plot = fig.add_subplot(grid[1, 3])\n",
    "    #grad_norm = [np.linalg.norm(gamma) for gamma in logger[\"grad_gamma\"]]\n",
    "    grad_plot.semilogy(range(len(logger[\"grad_gamma_norm\"])), logger[\"grad_gamma_norm\"], '-ob', label=method)\n",
    "    grad_plot.set_xlabel(\"Number of iteration\")\n",
    "    grad_plot.set_ylabel(\"Gradient norm\")\n",
    "    ylims = grad_plot.get_ylim()\n",
    "    grad_plot.set_ylim(min(tol/10, ylims[0]), ylims[1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"True beta: %s\"%(\" \".join(\"%.2f\"%s for s in beta.tolist())))\n",
    "    print(\"Model beta: %s\"%(\" \".join(\"%.2f\"%s for s in model.beta.tolist())))\n",
    "    if method == \"VariableProjectionNR\":\n",
    "        print(\"Model tbeta: %s\"%(\" \".join(\"%.2f\"%s for s in model.tbeta.tolist())))\n",
    "    print(\" \")\n",
    "    print(\"Empirical gamma: %s\"%(\" \".join(\"%.2f\"%s for s in empirical_gamma.tolist())))\n",
    "    print(\"Model gamma: %s\"%(\" \".join(\"%.2f\"%s for s in model.gamma.tolist())))\n",
    "    if method == \"VariableProjectionNR\":\n",
    "        print(\"Model tgamma: %s\"%(\" \".join(\"%.2f\"%s for s in model.tgamma.tolist())))\n",
    "    \n",
    "    launch_log.append(true_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4792ca955c0441999eb6ced529bcac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='EM', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=413, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='None',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,   # that's how much objects is going to be in test _for each study_\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1, # how close Z to X: Z := how_close*Z + (1-how_close)*X (need it for instability testing)                                                                    \n",
    "                 lb=0.1,     # \\lambda for regularizer ||beta - wobbly_beta||\n",
    "                 lg=1,       # \\lambda for regularizer ||gamma - wobbly_gamma||\n",
    "                 obs_std=0.1,  # diagonal elements of \\Lambda (noise variance)\n",
    "                 bootstrap='Nonparametric', \n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='None',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    # beta is [0, 1, 1, 1 ...]\n",
    "    # gamma is [0, 1, 1, ..., 1, 0]\n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[-1] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "     \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    # this is the best gamma one can have a hope to find\n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        # these are oracle and method which are capable of performing feature selection \n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "        # this says what regularizer we need to put in order to make the hessian PSD everywhere\n",
    "        # UPD: we don't use it, because we don't use NR anymore (we use GD instead)\n",
    "        # gamma_reg_exact_full, g_opt = train_oracle.good_lambda_gamma(mode=\"exact_full_hess\")\n",
    "        # print(r\"Suggested $\\lambda_\\gamma$ adjustment: \", gamma_reg_exact_full)\n",
    "        # print(r\"Used $\\lambda_\\gamma$: \", lg)\n",
    "    else:    \n",
    "        # these methods are left here for experimenting, they don't work for feature selection\n",
    "        # because they behave badly near the boundaries\n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        \n",
    "    # We start the feature selection process from a \"one EM iteration\" initial point\n",
    "    model.max_iter = 0\n",
    "    logger = model.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        beta0=np.ones(num_features),\n",
    "                        gamma0=np.ones(num_random_effects),\n",
    "                        initializer=\"EM\",\n",
    "                        )\n",
    "    model.max_iter = max_iter\n",
    "\n",
    "    # These are initial parameters for the very first iteration\n",
    "    parameters = {\n",
    "        (num_features + 1, num_random_effects): (model.beta, model.gamma, None)\n",
    "    }\n",
    "    \n",
    "    # k is how much non-zero elements of beta we want to get\n",
    "    for k in range(num_features, 0, -1):\n",
    "        train_oracle.k = k\n",
    "        test_oracle.k = k\n",
    "        prev_beta, *rest = parameters[(k + 1, k)]\n",
    "        tbeta = train_oracle.take_only_k_max(prev_beta, k)\n",
    "        \n",
    "        for j in range(k, 0, -1):\n",
    "            train_oracle.j = j\n",
    "            test_oracle.j = j\n",
    "            if j == k:\n",
    "                prev_beta, prev_gamma, *rest = parameters[(k + 1, k)]\n",
    "            else:\n",
    "                prev_beta, prev_gamma, *rest = parameters[(k, j + 1)]\n",
    "                \n",
    "            tgamma = prev_gamma\n",
    "            tgamma[tbeta==0] = 0\n",
    "            tgamma = train_oracle.take_only_k_max(tgamma, j)\n",
    "        \n",
    "            #print(k, j, \": \\n Beta: \", prev_beta, '\\n Gamma: ', prev_gamma, '\\n tbeta: ', tbeta, '\\n tgamma:', tgamma, '\\n')\n",
    "            logger = model.fit(train_oracle,\n",
    "                               test_oracle,\n",
    "                               beta0=prev_beta,\n",
    "                               gamma0=prev_gamma,\n",
    "                               tbeta=tbeta,\n",
    "                               tgamma=tgamma,\n",
    "                                method=method,\n",
    "                               initializer=None,\n",
    "                               use_line_search=True\n",
    "                               )\n",
    "            #print(k, j, \" after: \\n Beta: \", model.beta, '\\n Gamma: ', model.gamma, '\\n tbeta: ', model.tbeta, '\\n tgamma:', model.tgamma, '\\n')\n",
    "\n",
    "            train_loss = train_oracle.loss_reg(model.beta, model.gamma, tbeta, tgamma)\n",
    "            test_loss = test_oracle.loss_reg(model.beta, model.gamma, tbeta, tgamma)\n",
    "            if not logger[\"converged\"]:\n",
    "                parameters[(k, j)] = (prev_beta, prev_gamma, tbeta, tgamma, logger, train_loss, test_loss)\n",
    "            else:\n",
    "                parameters[(k, j)] = (model.beta, model.gamma, model.tbeta, model.tgamma, logger, train_loss, test_loss)\n",
    "            \n",
    "        \n",
    "    train_error = np.zeros((num_features, num_features))\n",
    "    test_error = np.zeros((num_features, num_features))\n",
    "    converged = np.zeros((num_features, num_features))\n",
    "    coefficients = np.zeros((num_features*num_features, 2*num_features))\n",
    "    dense_coefficients = np.zeros((num_features*num_features, 2*num_features))\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            beta, gamma, tbeta, tgamma, logger, train_loss, test_loss = parameters[(k, j)]\n",
    "            train_error[k-1, j-1] = train_loss\n",
    "            test_error[k-1, j-1] = test_loss\n",
    "            converged[k-1, j-1] = logger[\"converged\"]\n",
    "            \n",
    "            dense_coefficients[(k-1)*num_features:k*num_features, 2*(j-1)] = beta\n",
    "            dense_coefficients[(k-1)*num_features:k*num_features, 2*(j-1)+1] = gamma\n",
    "\n",
    "            coefficients[(k-1)*num_features:k*num_features, 2*(j-1)] = tbeta\n",
    "            coefficients[(k-1)*num_features:k*num_features, 2*(j-1)+1] = tgamma\n",
    "    \n",
    "    coefficients[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    coefficients[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    dense_coefficients[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    dense_coefficients[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    print(\"Convergence:\")\n",
    "    print(converged.T, '\\n')\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "    \n",
    "    dense_coefficients_plot = fig.add_subplot(grid[:2, :])\n",
    "    dense_coefficients_plot.imshow(dense_coefficients.T)\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            for l in range(num_features):\n",
    "                coef_beta = dense_coefficients[(k-1)*num_features+l, 2*(j-1)]\n",
    "                coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                coef_gamma = dense_coefficients[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                dense_coefficients_plot.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                dense_coefficients_plot.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    for l in range(num_features):\n",
    "        coef_beta = dense_coefficients[l, 2*(num_features-1)]\n",
    "        coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "        coef_gamma = dense_coefficients[l, 2*(num_features-1)+1]\n",
    "        coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "        dense_coefficients_plot.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "        dense_coefficients_plot.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "        \n",
    "    dense_coefficients_plot.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "    dense_coefficients_plot.set_xticklabels(np.arange(num_features)+1)\n",
    "    dense_coefficients_plot.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "    dense_coefficients_plot.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "    dense_coefficients_plot.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    dense_coefficients_plot.set_title(\"Betas and Gammas (probably dense solutions)\")\n",
    "    \n",
    "    ### Sparse coefficients\n",
    "    coefficients_plot = fig.add_subplot(grid[2:4, :])\n",
    "    coefficients_plot.imshow(coefficients.T)\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            for l in range(num_features):\n",
    "                coef_beta = coefficients[(k-1)*num_features+l, 2*(j-1)]\n",
    "                coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                coef_gamma = coefficients[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                coefficients_plot.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                coefficients_plot.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    for l in range(num_features):\n",
    "        coef_beta = coefficients[l, 2*(num_features-1)]\n",
    "        coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "        coef_gamma = coefficients[l, 2*(num_features-1)+1]\n",
    "        coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "        coefficients_plot.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "        coefficients_plot.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "        \n",
    "    coefficients_plot.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "    coefficients_plot.set_xticklabels(np.arange(num_features)+1)\n",
    "    coefficients_plot.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "    coefficients_plot.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "    coefficients_plot.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    coefficients_plot.set_title(\"Wobbly Betas and Gammas (sparse solutions)\")\n",
    "\n",
    "\n",
    "    #coefficients_plot.set_ylim((0, 2*num_features))\n",
    "    \n",
    "    #print(train_error)\n",
    "    #print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f629e6e8a85a40e2bcba644eec659368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='None', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=413, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=10, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='None',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,   # that's how much objects is going to be in test _for each study_\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1, # how close Z to X: Z := how_close*Z + (1-how_close)*X (need it for instability testing)                                                                    \n",
    "                 lb=0.1,     # \\lambda for regularizer ||beta - wobbly_beta||\n",
    "                 lg=1,       # \\lambda for regularizer ||gamma - wobbly_gamma||\n",
    "                 obs_std=0.1,  # diagonal elements of \\Lambda (noise variance)\n",
    "                 bootstrap='Nonparametric', \n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='None',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "    max_iter_t = 100\n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    # beta is [0, 1, 1, 1 ...]\n",
    "    # gamma is [0, 1, 1, ..., 1, 0]\n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[-1] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "     \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    # this is the best gamma one can have a hope to find\n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        # these are oracle and method which are capable of performing feature selection \n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "    else:    \n",
    "        # these methods are left here for experimenting, they don't work for feature selection\n",
    "        # because they behave badly near the boundaries\n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        \n",
    "    # We start the feature selection process from a \"one EM iteration\" initial point\n",
    "    model.max_iter = 0\n",
    "    logger = model.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        beta0=np.ones(num_features),\n",
    "                        gamma0=np.ones(num_random_effects),\n",
    "                        initializer=\"EM\",\n",
    "                        )\n",
    "    beta0 = model.beta\n",
    "    gamma0 = model.gamma\n",
    "    \n",
    "    # Or we start it at some bad point\n",
    "    beta0 = np.ones(num_features)/2\n",
    "    gamma0 = np.ones(num_random_effects)/2\n",
    "    \n",
    "    model.max_iter = max_iter\n",
    "\n",
    "    feature_selector = FeatureSelectorV1(max_iter=max_iter_t, tol=tol)\n",
    "    feature_selector.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        model,\n",
    "                        beta0=beta0,\n",
    "                        gamma0=gamma0,\n",
    "                        logger_keys=('loss', 'test_loss', 'proj_grad_gamma_norm', \"converged\", \n",
    "                                    'loss_wo_reg', 'loss_wo_reg_tgamma', 'test_loss_wo_reg', 'test_loss_wo_reg_tgamma'),\n",
    "                        method=\"VariableProjectionGD\", \n",
    "                        use_line_search=True,\n",
    "                        selection_mode=None)\n",
    "    \n",
    "    train_error = feature_selector.get_aggregated_parameter(\"loss\")\n",
    "    test_error = feature_selector.get_aggregated_parameter(\"test_loss\")\n",
    "    dense_loss_train = feature_selector.get_aggregated_parameter(\"loss_wo_reg\")\n",
    "    sparse_loss_train = feature_selector.get_aggregated_parameter(\"loss_wo_reg_tgamma\")\n",
    "    dense_loss_test = feature_selector.get_aggregated_parameter(\"test_loss_wo_reg\")\n",
    "    sparse_loss_test = feature_selector.get_aggregated_parameter(\"test_loss_wo_reg_tgamma\")\n",
    "    \n",
    "    converged = feature_selector.get_aggregated_parameter(\"converged\")\n",
    "    inner_iters = feature_selector.get_aggregated_parameter(\"inner_iters_total\")\n",
    "    all_betas = feature_selector.get_aggregated_parameter(\"beta\")\n",
    "    all_gammas = feature_selector.get_aggregated_parameter(\"gamma\")\n",
    "    all_tbetas = feature_selector.get_aggregated_parameter(\"tbeta\")\n",
    "    all_tgammas = feature_selector.get_aggregated_parameter(\"tgamma\")\n",
    "    dense_coefficients_flattened = np.zeros((num_features*num_features, 2*num_features))\n",
    "    sparse_coefficients_flattened = np.zeros((num_features*num_features, 2*num_features))\n",
    "    \n",
    "    # flattening coefficiens matrix (from 4d to 2d) \n",
    "    # TODO: there is probably an easier way through .reshape\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            dense_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)] = all_betas[k-1, j-1]\n",
    "            dense_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)+1] = all_gammas[k-1, j-1]\n",
    "            sparse_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)] = all_tbetas[k-1, j-1]\n",
    "            sparse_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)+1] = all_tgammas[k-1, j-1]\n",
    "    \n",
    "    # we put the true coefficients to the bottom-left corner of this matrix for both beta/gamma and tbeta/tgamma\n",
    "    dense_coefficients_flattened[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    dense_coefficients_flattened[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    sparse_coefficients_flattened[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    sparse_coefficients_flattened[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    \n",
    "    # printing the convergence matrix\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"Convergence:\")\n",
    "    print(converged.T, '\\n')\n",
    "    print(\"Internal convergence:\")\n",
    "    print(inner_iters.T, '\\n')\n",
    "    print(\"Loss:\")\n",
    "    print(train_error.T, '\\n')\n",
    "    print(\"Pure Dense Loss:\")\n",
    "    print(dense_loss_train.T, '\\n')\n",
    "    print(\"Pure Sparse Loss:\")\n",
    "    print(sparse_loss_train.T, '\\n')\n",
    "    \n",
    "    # creating the main canvas and a grid for all the subplotas\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "        \n",
    "    def plot_all_model_coefficients(coefs, ax):\n",
    "        ax.imshow(coefs.T)\n",
    "        for k in range(num_features, 0, -1):\n",
    "            for j in range(k, 0, -1):\n",
    "                for l in range(num_features):\n",
    "                    coef_beta = coefs[(k-1)*num_features+l, 2*(j-1)]\n",
    "                    coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                    coef_gamma = coefs[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                    coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                    ax.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                    ax.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        for l in range(num_features):\n",
    "            coef_beta = coefs[l, 2*(num_features-1)]\n",
    "            coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "            coef_gamma = coefs[l, 2*(num_features-1)+1]\n",
    "            coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "            ax.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "            ax.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "\n",
    "        ax.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "        ax.set_xticklabels(np.arange(num_features)+1)\n",
    "        ax.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "        ax.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "        ax.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # filling up the plot for beta/gamma\n",
    "    dense_coefficients_plot = fig.add_subplot(grid[:2, :])\n",
    "    dense_coefficients_plot.set_title(\"Betas and Gammas (probably dense solutions)\")\n",
    "    plot_all_model_coefficients(dense_coefficients_flattened, dense_coefficients_plot)\n",
    "    \n",
    "    # filling up the plot for tbeta/tgamma\n",
    "    sparse_coefficients_plot = fig.add_subplot(grid[2:4, :])\n",
    "    sparse_coefficients_plot.set_title(\"Wobbly Betas and Gammas (sparse solutions)\")\n",
    "    plot_all_model_coefficients(sparse_coefficients_flattened, sparse_coefficients_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d27fccb773433f9c9cbdc38b81740c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='None', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_error = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "num_features = 3\n",
    "\n",
    "figsize=(16, 16)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "train_error_plot = fig.add_subplot(grid[:2, :2])\n",
    "train_error_plot.imshow(train_error)\n",
    "#train_error_plot.set_xticks(np.arange(num_features))\n",
    "#train_error_plot.set_yticks(np.arange(num_features))\n",
    "\n",
    "for k in range(num_features, 0, -1):\n",
    "    for j in range(k, 0, -1):\n",
    "        text = train_error_plot.text(j-1, k-1, \"%.2f\"%train_error[k-1, j-1], ha=\"center\", va=\"center\", color=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: A= A1 , B= B1\n",
      "g: B =  triplekek , C= C1\n"
     ]
    }
   ],
   "source": [
    "def f(A=\"lol\", B=\"Kek_f\", **kwargs):\n",
    "    print(\"f: A=\", A, \", B=\", B)\n",
    "    g(B = \"triplekek\", **kwargs)\n",
    "    \n",
    "def g(B=\"Kek_g\", C=\"Cheburek\"):\n",
    "    print(\"g: B = \", B, \", C=\", C)\n",
    "    \n",
    "f(A = \"A1\", B=\"B1\", C=\"C1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
