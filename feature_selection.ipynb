{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import threading\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipyparallel import Client \n",
    "from tqdm import tqdm\n",
    "\n",
    "from lib.oracles import LinearLMEOracle, LinearLMEOracleRegularized\n",
    "from lib.solvers import LinearLMESolver, LinearLMERegSolver\n",
    "from lib.problems import LinearLMEProblem\n",
    "from lib.feature_selection import FeatureSelectorV1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LME Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=212, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=10, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=10, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-6, max=9, value=1e-5, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-10, max=9, value=0.1, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='EM',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1,\n",
    "                 lb=0.1,\n",
    "                 lg=1,\n",
    "                 obs_std=0.1, \n",
    "                 bootstrap='Nonparametric',\n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='EM',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[0] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "        \n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "\n",
    "        \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        #gamma_reg_exact_full, g_opt = train_oracle.good_lambda_gamma(mode=\"exact_full_hess\")\n",
    "        #gamma_reg_exact_full = lg\n",
    "        #train_oracle.lg = gamma_reg_exact_full\n",
    "        #test_oracle.lg = gamma_reg_exact_full\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "        logger = model.fit(train_oracle, test_oracle, method=method, initializer=initializer)\n",
    "        #print(logger[\"grad_gamma_norm\"])\n",
    "    else:    \n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        logger = model.fit(train_oracle, test_oracle, method=method, initializer=initializer)\n",
    "    \n",
    "    # Eigenvalues and eigenvectors at the solution\n",
    "    #print(train_oracle.gradient_gamma(beta, empirical_gamma))\n",
    "    #print(np.linalg.eigvals(train_oracle.hessian_gamma(beta, empirical_gamma)))\n",
    "    \n",
    "    if not logger[\"converged\"]:\n",
    "        print(\"Did not converge\") \n",
    "        \n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "    prediction_plot = fig.add_subplot(grid[:2, :2])\n",
    "\n",
    "\n",
    "    model_parameters_plot = fig.add_subplot(grid[0, 2])\n",
    "    pred_beta = model.beta\n",
    "    model_parameters_plot.scatter(true_parameters['beta'], pred_beta)\n",
    "    model_parameters_plot.set_xlabel(\"True parameters\")\n",
    "    model_parameters_plot.set_ylabel(\"Inferred parameters\")\n",
    "    model_parameters_low_lim = -0.5#min(min(pred_beta), min(true_parameters['beta'])) - 0.1\n",
    "    model_parameters_high_lim = 1.5#max(max(pred_beta), max(true_parameters['beta'])) + 0.1\n",
    "    model_parameters_plot.set_xlim(model_parameters_low_lim, model_parameters_high_lim)\n",
    "    model_parameters_plot.set_ylim(model_parameters_low_lim, model_parameters_high_lim)\n",
    "    model_parameters_plot.plot(model_parameters_plot.get_xlim(), model_parameters_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    \n",
    "    \n",
    "    random_effects_plot = fig.add_subplot(grid[1, 2])\n",
    "    random_effects_low_lim = 0\n",
    "    random_effects_high_lim = 0\n",
    "    for i, (u_pred, u_true) in enumerate(zip(model.us, true_parameters['random_effects'])):\n",
    "        random_effects_plot.scatter(u_true, u_pred, label=\"Study %d\" % (i + 1), c=color_map[i])\n",
    "        random_effects_low_lim = min(min(u_true), min(u_pred), random_effects_low_lim)\n",
    "        random_effects_high_lim = max(max(u_true), max(u_pred), random_effects_high_lim)\n",
    "    random_effects_plot.legend()\n",
    "    random_effects_low_lim -= 0.1\n",
    "    random_effects_high_lim += 0.1\n",
    "    random_effects_plot.set_xlim(random_effects_low_lim, random_effects_high_lim)\n",
    "    random_effects_plot.set_ylim(random_effects_low_lim, random_effects_high_lim)\n",
    "    random_effects_plot.plot(random_effects_plot.get_xlim(), random_effects_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    random_effects_plot.set_xlabel(\"True random effects\")\n",
    "    random_effects_plot.set_ylabel(\"Inferred random effects\")\n",
    "\n",
    "    gamma_to_show = model.gamma\n",
    "    gamma_plot = fig.add_subplot(grid[0, 3])\n",
    "    gamma_plot.scatter(empirical_gamma, gamma_to_show, label=\"Inferred\")\n",
    "    gamma_low_lim = min(min(empirical_gamma), min(gamma_to_show)) - 0.2\n",
    "    gamma_high_lim = max(max(empirical_gamma), max(gamma_to_show)) + 0.2\n",
    "    gamma_plot.set_xlim(gamma_low_lim, gamma_high_lim)\n",
    "    gamma_plot.set_ylim(gamma_low_lim, gamma_high_lim)\n",
    "    gamma_plot.plot(gamma_plot.get_xlim(), gamma_plot.get_ylim(), ls='--', c=\".3\")\n",
    "    gamma_plot.set_xlabel(\"Empirical gamma\")\n",
    "    gamma_plot.set_ylabel(\"Inferred gamma\")\n",
    "\n",
    "    abs_min = np.inf\n",
    "    if num_random_effects == 2:\n",
    "        trajectory_plot = fig.add_subplot(grid[2:4, 0:2])\n",
    "    \n",
    "        xlims = [0, 2] #ax.get_xlim()\n",
    "        ylims = [0, 2] #ax.get_ylim()\n",
    "\n",
    "        plot_resolution = 100\n",
    "        x = np.linspace(0, xlims[1], plot_resolution)\n",
    "        y = np.linspace(0, ylims[1], plot_resolution)\n",
    "        z = np.zeros((plot_resolution, plot_resolution))\n",
    "        zh = np.zeros((plot_resolution, plot_resolution))\n",
    "        def psd(hessian):\n",
    "            eigvals = np.linalg.eigvals(hessian)\n",
    "            if np.linalg.norm(np.imag(eigvals)) > 1e-15:\n",
    "                return -1\n",
    "            min_eigval = min(np.real(eigvals))\n",
    "            if min_eigval < 0:\n",
    "                return -1\n",
    "            else:\n",
    "                return min_eigval\n",
    "        for i, g2 in enumerate(y):\n",
    "            for j, g1 in enumerate(x):\n",
    "                gamma0 = np.array([g1, g2])\n",
    "                beta0 = train_oracle.optimal_beta(gamma0)\n",
    "                z[i, j] = train_oracle.loss(beta0, gamma0)\n",
    "                hessian = train_oracle.hessian_gamma(beta0, gamma0)\n",
    "                zh[i, j] = psd(hessian)\n",
    "\n",
    "        abs_min = min(min(logger[\"loss\"]), np.min(z)) - 1e-8\n",
    "\n",
    "        csh = trajectory_plot.contourf(x, y, zh, levels=[0, 1e13], colors=\"lightgreen\")\n",
    "\n",
    "        levels = np.min(z) + np.array([1e-2, 1e-1, 1e0, 1e1, 1e2])\n",
    "        levels_labels = [\"1e-2\", \"1e-1\", \"1e0\", \"1e1\", \"1e2\"]\n",
    "        levels_dict = {levels[i]: levels_labels[i] for i in range(len(levels_labels))}\n",
    "\n",
    "        cs = trajectory_plot.contour(x, y, z, levels=levels)\n",
    "        plt.clabel(cs, fontsize=8, fmt=levels_dict)\n",
    "\n",
    "\n",
    "        gamma_trace = np.array(logger[\"gamma\"]).T\n",
    "        trajectory_plot.plot(gamma_trace[0], gamma_trace[1], '-ob', label = method)\n",
    "\n",
    "        trajectory_plot.set_xlim(xlims)\n",
    "        trajectory_plot.set_ylim(ylims)\n",
    "        trajectory_plot.set_xlabel(r\"$\\gamma_1$, variation of the first random effect\")\n",
    "        trajectory_plot.set_ylabel(r\"$\\gamma_2$, variation of the second random effect\")\n",
    "        \n",
    "        extra_plot = fig.add_subplot(grid[2:4, 2:4])\n",
    "        \n",
    "        def psd_criterion(gamma_t):\n",
    "            beta_t = train_oracle.optimal_beta(gamma_t)\n",
    "            criterion_satisfied = []\n",
    "            gamma_mat = np.diag(gamma_t)\n",
    "            for x, y, z, l in train:\n",
    "                omega = z.dot(gamma_mat).dot(z.T) + l\n",
    "                xi = y - x.dot(beta_t)\n",
    "                criterion_satisfied.append(xi.T.dot(np.linalg.inv(omega)).dot(xi) <= 1/2)\n",
    "            return sum(criterion_satisfied)\n",
    "        \n",
    "        zc = np.zeros((plot_resolution, plot_resolution))\n",
    "        for i, g2 in enumerate(y):\n",
    "            for j, g1 in enumerate(x):\n",
    "                gamma_t = np.array([g1, g2])\n",
    "                zc[i, j] = psd_criterion(gamma_t)\n",
    "                                \n",
    "        cc = extra_plot.contourf(x, y, zc)\n",
    "        cs = extra_plot.contour(x, y, z, levels=levels)\n",
    "        plt.clabel(cs, fontsize=8, fmt=levels_dict)\n",
    "        extra_plot.set_xlim(xlims)\n",
    "        extra_plot.set_ylim(ylims)\n",
    "        extra_plot.set_xlabel(r\"$\\gamma_1$, variation of the first random effect\")\n",
    "        extra_plot.set_ylabel(r\"$\\gamma_2$, variation of the second random effect\")\n",
    "\n",
    "    #loss_plot = fig.add_subplot(grid[1, 3])\n",
    "    #min_loss = min(np.min(logger[\"loss\"]), abs_min)\n",
    "    #loss_plot.semilogy(range(len(logger[\"loss\"])), logger[\"loss\"] - min_loss + 1e-8, '-ob', label=method)\n",
    "    #loss_plot.set_ylim((1e-8, 1e3))\n",
    "    \n",
    "    grad_plot = fig.add_subplot(grid[1, 3])\n",
    "    #grad_norm = [np.linalg.norm(gamma) for gamma in logger[\"grad_gamma\"]]\n",
    "    grad_plot.semilogy(range(len(logger[\"grad_gamma_norm\"])), logger[\"grad_gamma_norm\"], '-ob', label=method)\n",
    "    grad_plot.set_xlabel(\"Number of iteration\")\n",
    "    grad_plot.set_ylabel(\"Gradient norm\")\n",
    "    ylims = grad_plot.get_ylim()\n",
    "    grad_plot.set_ylim(min(tol/10, ylims[0]), ylims[1])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"True beta: %s\"%(\" \".join(\"%.2f\"%s for s in beta.tolist())))\n",
    "    print(\"Model beta: %s\"%(\" \".join(\"%.2f\"%s for s in model.beta.tolist())))\n",
    "    if method == \"VariableProjectionNR\":\n",
    "        print(\"Model tbeta: %s\"%(\" \".join(\"%.2f\"%s for s in model.tbeta.tolist())))\n",
    "    print(\" \")\n",
    "    print(\"Empirical gamma: %s\"%(\" \".join(\"%.2f\"%s for s in empirical_gamma.tolist())))\n",
    "    print(\"Model gamma: %s\"%(\" \".join(\"%.2f\"%s for s in model.gamma.tolist())))\n",
    "    if method == \"VariableProjectionNR\":\n",
    "        print(\"Model tgamma: %s\"%(\" \".join(\"%.2f\"%s for s in model.tgamma.tolist())))\n",
    "    \n",
    "    launch_log.append(true_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db3d39a64154046931a6d06485c324d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='EM', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=413, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='None',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,   # that's how much objects is going to be in test _for each study_\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1, # how close Z to X: Z := how_close*Z + (1-how_close)*X (need it for instability testing)                                                                    \n",
    "                 lb=0.1,     # \\lambda for regularizer ||beta - wobbly_beta||\n",
    "                 lg=1,       # \\lambda for regularizer ||gamma - wobbly_gamma||\n",
    "                 obs_std=0.1,  # diagonal elements of \\Lambda (noise variance)\n",
    "                 bootstrap='Nonparametric', \n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='None',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    # beta is [0, 1, 1, 1 ...]\n",
    "    # gamma is [0, 1, 1, ..., 1, 0]\n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[-1] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "     \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    # this is the best gamma one can have a hope to find\n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        # these are oracle and method which are capable of performing feature selection \n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "        # this says what regularizer we need to put in order to make the hessian PSD everywhere\n",
    "        # UPD: we don't use it, because we don't use NR anymore (we use GD instead)\n",
    "        # gamma_reg_exact_full, g_opt = train_oracle.good_lambda_gamma(mode=\"exact_full_hess\")\n",
    "        # print(r\"Suggested $\\lambda_\\gamma$ adjustment: \", gamma_reg_exact_full)\n",
    "        # print(r\"Used $\\lambda_\\gamma$: \", lg)\n",
    "    else:    \n",
    "        # these methods are left here for experimenting, they don't work for feature selection\n",
    "        # because they behave badly near the boundaries\n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        \n",
    "    # We start the feature selection process from a \"one EM iteration\" initial point\n",
    "    model.max_iter = 0\n",
    "    logger = model.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        beta0=np.ones(num_features),\n",
    "                        gamma0=np.ones(num_random_effects),\n",
    "                        initializer=\"EM\",\n",
    "                        )\n",
    "    model.max_iter = max_iter\n",
    "\n",
    "    # These are initial parameters for the very first iteration\n",
    "    parameters = {\n",
    "        (num_features + 1, num_random_effects): (model.beta, model.gamma, None)\n",
    "    }\n",
    "    \n",
    "    # k is how much non-zero elements of beta we want to get\n",
    "    for k in range(num_features, 0, -1):\n",
    "        train_oracle.k = k\n",
    "        test_oracle.k = k\n",
    "        prev_beta, *rest = parameters[(k + 1, k)]\n",
    "        tbeta = train_oracle.take_only_k_max(prev_beta, k)\n",
    "        \n",
    "        for j in range(k, 0, -1):\n",
    "            train_oracle.j = j\n",
    "            test_oracle.j = j\n",
    "            if j == k:\n",
    "                prev_beta, prev_gamma, *rest = parameters[(k + 1, k)]\n",
    "            else:\n",
    "                prev_beta, prev_gamma, *rest = parameters[(k, j + 1)]\n",
    "                \n",
    "            tgamma = prev_gamma\n",
    "            tgamma[tbeta==0] = 0\n",
    "            tgamma = train_oracle.take_only_k_max(tgamma, j)\n",
    "        \n",
    "            #print(k, j, \": \\n Beta: \", prev_beta, '\\n Gamma: ', prev_gamma, '\\n tbeta: ', tbeta, '\\n tgamma:', tgamma, '\\n')\n",
    "            logger = model.fit(train_oracle,\n",
    "                               test_oracle,\n",
    "                               beta0=prev_beta,\n",
    "                               gamma0=prev_gamma,\n",
    "                               tbeta=tbeta,\n",
    "                               tgamma=tgamma,\n",
    "                                method=method,\n",
    "                               initializer=None,\n",
    "                               use_line_search=True\n",
    "                               )\n",
    "            #print(k, j, \" after: \\n Beta: \", model.beta, '\\n Gamma: ', model.gamma, '\\n tbeta: ', model.tbeta, '\\n tgamma:', model.tgamma, '\\n')\n",
    "\n",
    "            train_loss = train_oracle.loss_reg(model.beta, model.gamma, tbeta, tgamma)\n",
    "            test_loss = test_oracle.loss_reg(model.beta, model.gamma, tbeta, tgamma)\n",
    "            if not logger[\"converged\"]:\n",
    "                parameters[(k, j)] = (prev_beta, prev_gamma, tbeta, tgamma, logger, train_loss, test_loss)\n",
    "            else:\n",
    "                parameters[(k, j)] = (model.beta, model.gamma, model.tbeta, model.tgamma, logger, train_loss, test_loss)\n",
    "            \n",
    "        \n",
    "    train_error = np.zeros((num_features, num_features))\n",
    "    test_error = np.zeros((num_features, num_features))\n",
    "    converged = np.zeros((num_features, num_features))\n",
    "    coefficients = np.zeros((num_features*num_features, 2*num_features))\n",
    "    dense_coefficients = np.zeros((num_features*num_features, 2*num_features))\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            beta, gamma, tbeta, tgamma, logger, train_loss, test_loss = parameters[(k, j)]\n",
    "            train_error[k-1, j-1] = train_loss\n",
    "            test_error[k-1, j-1] = test_loss\n",
    "            converged[k-1, j-1] = logger[\"converged\"]\n",
    "            \n",
    "            dense_coefficients[(k-1)*num_features:k*num_features, 2*(j-1)] = beta\n",
    "            dense_coefficients[(k-1)*num_features:k*num_features, 2*(j-1)+1] = gamma\n",
    "\n",
    "            coefficients[(k-1)*num_features:k*num_features, 2*(j-1)] = tbeta\n",
    "            coefficients[(k-1)*num_features:k*num_features, 2*(j-1)+1] = tgamma\n",
    "    \n",
    "    coefficients[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    coefficients[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    dense_coefficients[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    dense_coefficients[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    print(\"Convergence:\")\n",
    "    print(converged.T, '\\n')\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "    \n",
    "    dense_coefficients_plot = fig.add_subplot(grid[:2, :])\n",
    "    dense_coefficients_plot.imshow(dense_coefficients.T)\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            for l in range(num_features):\n",
    "                coef_beta = dense_coefficients[(k-1)*num_features+l, 2*(j-1)]\n",
    "                coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                coef_gamma = dense_coefficients[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                dense_coefficients_plot.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                dense_coefficients_plot.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    for l in range(num_features):\n",
    "        coef_beta = dense_coefficients[l, 2*(num_features-1)]\n",
    "        coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "        coef_gamma = dense_coefficients[l, 2*(num_features-1)+1]\n",
    "        coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "        dense_coefficients_plot.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "        dense_coefficients_plot.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "        \n",
    "    dense_coefficients_plot.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "    dense_coefficients_plot.set_xticklabels(np.arange(num_features)+1)\n",
    "    dense_coefficients_plot.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "    dense_coefficients_plot.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "    dense_coefficients_plot.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    dense_coefficients_plot.set_title(\"Betas and Gammas (probably dense solutions)\")\n",
    "    \n",
    "    ### Sparse coefficients\n",
    "    coefficients_plot = fig.add_subplot(grid[2:4, :])\n",
    "    coefficients_plot.imshow(coefficients.T)\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            for l in range(num_features):\n",
    "                coef_beta = coefficients[(k-1)*num_features+l, 2*(j-1)]\n",
    "                coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                coef_gamma = coefficients[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                coefficients_plot.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                coefficients_plot.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    for l in range(num_features):\n",
    "        coef_beta = coefficients[l, 2*(num_features-1)]\n",
    "        coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "        coef_gamma = coefficients[l, 2*(num_features-1)+1]\n",
    "        coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "        coefficients_plot.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "        coefficients_plot.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "        \n",
    "    coefficients_plot.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "    coefficients_plot.set_xticklabels(np.arange(num_features)+1)\n",
    "    coefficients_plot.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "    coefficients_plot.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "    coefficients_plot.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    coefficients_plot.set_title(\"Wobbly Betas and Gammas (sparse solutions)\")\n",
    "\n",
    "\n",
    "    #coefficients_plot.set_ylim((0, 2*num_features))\n",
    "    \n",
    "    #print(train_error)\n",
    "    #print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f629e6e8a85a40e2bcba644eec659368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='None', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_sizes_widget = widgets.Text(value='300, 100, 50', placeholder=\"100, 100, 100\", description='Study sizes:')\n",
    "test_widget = widgets.IntSlider(min=1, max=20, value=10, step=1, description=\"Test size\")\n",
    "random_seed_widget = widgets.IntSlider(min=1, max=1000, value=413, step=1, description=\"Random seed\")\n",
    "num_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Features\")\n",
    "num_random_features_widget = widgets.IntSlider(min=2, max=10, value=6, step=1, description=\"Random effects\")\n",
    "how_close_widget = widgets.FloatSlider(min=0, max=1, value=1, step=0.01, description=\"How close?\")\n",
    "obs_std_widget = widgets.FloatLogSlider(min=-6, max=2, value=5e-2, step=1, description=\"Random noise std\")\n",
    "l2_beta_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=1, step=1, description=\"L2 beta regularization\")\n",
    "l2_gamma_reg_widget = widgets.FloatLogSlider(min=-3, max=5, value=10, step=1, description=\"L2 gamma regularization\")\n",
    "features_widget = widgets.HBox([num_features_widget, num_random_features_widget])\n",
    "rest_widget = widgets.HBox([obs_std_widget, random_seed_widget])\n",
    "\n",
    "init_widget = widgets.ToggleButtons(options=['None', 'EM'], \n",
    "                                         value='None',\n",
    "                                         description='Initializer:')\n",
    "method_widget = widgets.ToggleButtons(options=['GradDescent', 'NewtonRaphson', 'EM', 'VariableProjectionGD'], \n",
    "                                         value='VariableProjectionGD',\n",
    "                                         description='Method:')\n",
    "bootstrap_widget = widgets.ToggleButtons(options=['None', 'Parametric', 'Nonparametric', 'Analytic'], \n",
    "                                         value='None',\n",
    "                                         description='Bootstrap:')\n",
    "bootstrap_capacity_widget = widgets.IntSlider(min=10, max=1000, value=100, step=10, description=\"Bootstrap samples\")\n",
    "launch_log = []\n",
    "\n",
    "def visualize_sample_problem(study_sizes=\"100, 100, 100\",\n",
    "                 test_size=5,   # that's how much objects is going to be in test _for each study_\n",
    "                 num_features=6, \n",
    "                 num_random_effects=6,\n",
    "                 how_close=1, # how close Z to X: Z := how_close*Z + (1-how_close)*X (need it for instability testing)                                                                    \n",
    "                 lb=0.1,     # \\lambda for regularizer ||beta - wobbly_beta||\n",
    "                 lg=1,       # \\lambda for regularizer ||gamma - wobbly_gamma||\n",
    "                 obs_std=0.1,  # diagonal elements of \\Lambda (noise variance)\n",
    "                 bootstrap='Nonparametric', \n",
    "                 bootstrap_capacity=100,\n",
    "                 initializer='None',\n",
    "                 method='NewtonRaphson',\n",
    "                 random_seed=42):\n",
    "    \n",
    "    figsize=(16, 16)\n",
    "    tol = 1e-4\n",
    "    max_iter = 1000\n",
    "    max_iter_t = 100\n",
    "    \n",
    "    if study_sizes == \"\":\n",
    "        study_sizes = [study1, study2, study3]\n",
    "    else:\n",
    "        study_sizes = [int(s) for s in study_sizes.split(\", \")]\n",
    "    test_study_sizes = [test_size]*len(study_sizes)\n",
    "    \n",
    "    # beta is [0, 1, 1, 1 ...]\n",
    "    # gamma is [0, 1, 1, ..., 1, 0]\n",
    "    beta = np.ones(num_features)\n",
    "    gamma = np.ones(num_random_effects)\n",
    "    \n",
    "    beta[-1] = 0\n",
    "    gamma[0] = 0\n",
    "    gamma[-1] = 0\n",
    "     \n",
    "    train, beta, gamma, random_effects, errs = LinearLMEProblem.generate(study_sizes=study_sizes,\n",
    "                                                                         num_features=num_features,\n",
    "                                                                         beta=beta,\n",
    "                                                                         gamma=gamma,\n",
    "                                                                         num_random_effects=num_random_effects,\n",
    "                                                                         how_close_z_to_x=how_close,\n",
    "                                                                         obs_std=obs_std,\n",
    "                                                                         seed=random_seed)\n",
    "    \n",
    "    # this is the best gamma one can have a hope to find\n",
    "    empirical_gamma = np.sum(random_effects ** 2, axis=0) / len(study_sizes)\n",
    "    \n",
    "    test = LinearLMEProblem.generate(study_sizes=test_study_sizes, beta=beta, gamma=gamma,\n",
    "                                     how_close_z_to_x=how_close,\n",
    "                                     true_random_effects=random_effects,\n",
    "                                     seed=random_seed + 1, \n",
    "                                     obs_std=obs_std,\n",
    "                                     return_true_parameters=False)\n",
    "    true_parameters = {\n",
    "        \"beta\": beta,\n",
    "        \"gamma\": gamma,\n",
    "        \"random_effects\": random_effects,\n",
    "        \"errs\": errs,\n",
    "        \"train\": train,\n",
    "        \"test\": test,\n",
    "        \"seed\": random_seed\n",
    "    }\n",
    "    \n",
    "    color_map = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"cyan\", \"purple\", \"orange\"]\n",
    "    \n",
    "    \n",
    "    if method == \"VariableProjectionGD\":\n",
    "        # these are oracle and method which are capable of performing feature selection \n",
    "        train_oracle = LinearLMEOracleRegularized(train, lb=lb, lg=lg)\n",
    "        test_oracle = LinearLMEOracleRegularized(test, lb=lb, lg=lg)\n",
    "        model = LinearLMERegSolver(tol=tol, max_iter=max_iter)\n",
    "    else:    \n",
    "        # these methods are left here for experimenting, they don't work for feature selection\n",
    "        # because they behave badly near the boundaries\n",
    "        train_oracle = LinearLMEOracle(train)\n",
    "        test_oracle = LinearLMEOracle(test)\n",
    "        model = LinearLMESolver(tol=tol, max_iter=max_iter)\n",
    "        \n",
    "    # We start the feature selection process from a \"one EM iteration\" initial point\n",
    "    model.max_iter = 0\n",
    "    logger = model.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        beta0=np.ones(num_features),\n",
    "                        gamma0=np.ones(num_random_effects),\n",
    "                        initializer=\"EM\",\n",
    "                        )\n",
    "    beta0 = model.beta\n",
    "    gamma0 = model.gamma\n",
    "    \n",
    "    # Or we start it at some bad point\n",
    "    # beta0 = np.ones(num_features)/2\n",
    "    # gamma0 = np.ones(num_random_effects)/2\n",
    "    \n",
    "    model.max_iter = max_iter\n",
    "\n",
    "    feature_selector = FeatureSelectorV1(max_iter=max_iter_t, tol=tol)\n",
    "    feature_selector.fit(train_oracle,\n",
    "                        test_oracle,\n",
    "                        model,\n",
    "                        beta0=beta0,\n",
    "                        gamma0=gamma0,\n",
    "                        logger_keys=('loss', 'test_loss', 'proj_grad_gamma_norm', \"converged\", \n",
    "                                    'loss_wo_reg', 'loss_wo_reg_tgamma', 'test_loss_wo_reg', 'test_loss_wo_reg_tgamma'),\n",
    "                        method=\"VariableProjectionGD\", \n",
    "                        use_line_search=True,\n",
    "                        selection_mode=None)\n",
    "    \n",
    "    train_error = feature_selector.get_aggregated_parameter(\"loss\")\n",
    "    test_error = feature_selector.get_aggregated_parameter(\"test_loss\")\n",
    "    dense_loss_train = feature_selector.get_aggregated_parameter(\"loss_wo_reg\")\n",
    "    sparse_loss_train = feature_selector.get_aggregated_parameter(\"loss_wo_reg_tgamma\")\n",
    "    dense_loss_test = feature_selector.get_aggregated_parameter(\"test_loss_wo_reg\")\n",
    "    sparse_loss_test = feature_selector.get_aggregated_parameter(\"test_loss_wo_reg_tgamma\")\n",
    "    \n",
    "    converged = feature_selector.get_aggregated_parameter(\"converged\")\n",
    "    inner_iters = feature_selector.get_aggregated_parameter(\"inner_iters_total\")\n",
    "    all_betas = feature_selector.get_aggregated_parameter(\"beta\")\n",
    "    all_gammas = feature_selector.get_aggregated_parameter(\"gamma\")\n",
    "    all_tbetas = feature_selector.get_aggregated_parameter(\"tbeta\")\n",
    "    all_tgammas = feature_selector.get_aggregated_parameter(\"tgamma\")\n",
    "    dense_coefficients_flattened = np.zeros((num_features*num_features, 2*num_features))\n",
    "    sparse_coefficients_flattened = np.zeros((num_features*num_features, 2*num_features))\n",
    "    \n",
    "    # flattening coefficiens matrix (from 4d to 2d) \n",
    "    # TODO: there is probably an easier way through .reshape\n",
    "    for k in range(num_features, 0, -1):\n",
    "        for j in range(k, 0, -1):\n",
    "            dense_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)] = all_betas[k-1, j-1]\n",
    "            dense_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)+1] = all_gammas[k-1, j-1]\n",
    "            sparse_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)] = all_tbetas[k-1, j-1]\n",
    "            sparse_coefficients_flattened[(k-1)*num_features:k*num_features, 2*(j-1)+1] = all_tgammas[k-1, j-1]\n",
    "    \n",
    "    # we put the true coefficients to the bottom-left corner of this matrix for both beta/gamma and tbeta/tgamma\n",
    "    dense_coefficients_flattened[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    dense_coefficients_flattened[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    sparse_coefficients_flattened[:num_features, 2*(num_features-1)] = true_parameters[\"beta\"]\n",
    "    sparse_coefficients_flattened[:num_features, 2*(num_features-1)+1] = empirical_gamma\n",
    "    \n",
    "    \n",
    "    # printing the convergence matrix\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"Convergence:\")\n",
    "    print(converged.T, '\\n')\n",
    "    print(\"Internal convergence:\")\n",
    "    print(inner_iters.T, '\\n')\n",
    "    print(\"Loss:\")\n",
    "    print(train_error.T, '\\n')\n",
    "    print(\"Pure Dense Loss:\")\n",
    "    print(dense_loss_train.T, '\\n')\n",
    "    print(\"Pure Sparse Loss:\")\n",
    "    print(sparse_loss_train.T, '\\n')\n",
    "    \n",
    "    # creating the main canvas and a grid for all the subplotas\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(4, 4, wspace=0.3, hspace=0.3)\n",
    "        \n",
    "    def plot_all_model_coefficients(coefs, ax):\n",
    "        ax.imshow(coefs.T)\n",
    "        for k in range(num_features, 0, -1):\n",
    "            for j in range(k, 0, -1):\n",
    "                for l in range(num_features):\n",
    "                    coef_beta = coefs[(k-1)*num_features+l, 2*(j-1)]\n",
    "                    coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "                    coef_gamma = coefs[(k-1)*num_features+l, 2*(j-1)+1]\n",
    "                    coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "                    ax.text((k-1)*num_features+l, 2*(j-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "                    ax.text((k-1)*num_features+l, 2*(j-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        for l in range(num_features):\n",
    "            coef_beta = coefs[l, 2*(num_features-1)]\n",
    "            coef_beta_str = \"0\" if coef_beta == 0 else \"%.2f\"%coef_beta\n",
    "            coef_gamma = coefs[l, 2*(num_features-1)+1]\n",
    "            coef_gamma_str = \"0\" if coef_gamma == 0 else \"%.2f\"%coef_gamma\n",
    "            ax.text(l, 2*(num_features-1), coef_beta_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "            ax.text(l, 2*(num_features-1)+1, coef_gamma_str, ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "\n",
    "        ax.set_xticks(np.arange(-0.5, num_features*num_features, num_features))\n",
    "        ax.set_xticklabels(np.arange(num_features)+1)\n",
    "        ax.set_yticks(np.arange(-0.5, 2*num_features, 2))\n",
    "        ax.set_yticklabels(np.arange(1, num_features+1).tolist() + [\"\"])\n",
    "        ax.grid(which=\"major\", color='r', linestyle='-', linewidth=2)\n",
    "    \n",
    "    # filling up the plot for beta/gamma\n",
    "    dense_coefficients_plot = fig.add_subplot(grid[:2, :])\n",
    "    dense_coefficients_plot.set_title(\"Betas and Gammas (probably dense solutions)\")\n",
    "    plot_all_model_coefficients(dense_coefficients_flattened, dense_coefficients_plot)\n",
    "    \n",
    "    # filling up the plot for tbeta/tgamma\n",
    "    sparse_coefficients_plot = fig.add_subplot(grid[2:4, :])\n",
    "    sparse_coefficients_plot.set_title(\"Wobbly Betas and Gammas (sparse solutions)\")\n",
    "    plot_all_model_coefficients(sparse_coefficients_flattened, sparse_coefficients_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3112bd39baf418d86658ff938eea735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='300, 100, 50', description='Study sizes:', placeholder='100, 100, 100'), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_sample_problem(study_sizes='100, 100, 100', test_size=5, num_features=6, num_random_effects=6, how_close=1, lb=0.1, lg=1, obs_std=0.1, bootstrap='Nonparametric', bootstrap_capacity=100, initializer='None', method='NewtonRaphson', random_seed=42)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(visualize_sample_problem, \n",
    "                study_sizes = study_sizes_widget,\n",
    "                test_size = test_widget,\n",
    "                num_features = num_features_widget,\n",
    "                num_random_effects = num_random_features_widget,\n",
    "                how_close = how_close_widget,\n",
    "                lb=l2_beta_reg_widget,\n",
    "                lg=l2_gamma_reg_widget,\n",
    "                obs_std = obs_std_widget,\n",
    "                bootstrap = bootstrap_widget,\n",
    "                bootstrap_capacity = bootstrap_capacity_widget,\n",
    "                initializer = init_widget, \n",
    "                method = method_widget,\n",
    "                random_seed = random_seed_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating correlated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3877787807814457e-16\n",
      "[[ 1.22 -0.89 -0.99 -0.89 -0.68  0.72 -0.71  1.02 -0.91 -0.13]]\n",
      "[[ 0.47 -1.1  -1.32  0.38 -2.2  -0.34  1.2   1.51 -0.96  1.27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aksh/.anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "rho = 0.7\n",
    "A = np.random.randn(10, 1)\n",
    "B = np.random.randn(10, 1)\n",
    "B_ort = B - A*np.linalg.lstsq(A, B)[0][0, 0]\n",
    "print(sum(A*B_ort))\n",
    "C = rho*A + sqrt(1-rho**2)*B_ort\n",
    "print(A.T)\n",
    "print(C.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9780057911287948, 9.970171802832275e-07)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(B.T[0], C.T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "A = np.random.rand(3, 3)\n",
    "B = np.random.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19086771, 0.09291206, 0.52926729])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(A.dot(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19086771, 0.09291206, 0.52926729])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A*B.T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg.lapack import get_lapack_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.linalg.cholesky(A.T.dot(A))\n",
    "L_inv = get_lapack_funcs(\"trtri\")(L.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.41132895,  -0.91609052,  -3.00017273],\n",
       "       [  0.        ,   0.92021979, -22.88155172],\n",
       "       [  0.        ,   0.        ,  32.15762143]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.41132895,  -0.91609052,  -3.00017273],\n",
       "       [  0.        ,   0.92021979, -22.88155172],\n",
       "       [  0.        ,   0.        ,  32.15762143]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70855204, 0.        , 0.        ],\n",
       "       [0.70537257, 1.0866969 , 0.        ],\n",
       "       [0.56800835, 0.77323229, 0.03109683]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cholesky(A.T.dot(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nature of instability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c1eecf150>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 30\n",
    "n = 6\n",
    "eps = 1e-7\n",
    "np.random.seed(42)\n",
    "\n",
    "def f(alpha):\n",
    "    kernel = 0\n",
    "    kernel2 = 0\n",
    "    tail = 0\n",
    "    tail2 = 0\n",
    "    for _ in range(3):\n",
    "        X = np.random.rand(m, n)\n",
    "        Z = np.random.rand(m, n)\n",
    "        y = np.random.rand(m)\n",
    "        gamma = abs(np.random.randn(n))\n",
    "        Z = (1-alpha)*X + alpha*Z\n",
    "        Omega = Z.dot(np.diag(gamma)).dot(Z.T) + eps*np.eye(m)\n",
    "        L = np.linalg.cholesky(Omega)\n",
    "        L_inv = np.linalg.inv(L)\n",
    "        Lx = L_inv.dot(X)\n",
    "        Omega_inv = np.linalg.inv(Omega)\n",
    "        kernel += X.T.dot(Omega_inv).dot(X)\n",
    "        kernel2 += Lx.T.dot(Lx)\n",
    "        tail += X.T.dot(Omega_inv).dot(y)\n",
    "        tail2 += Lx.T.dot(L_inv).dot(y)\n",
    "    beta1 = np.linalg.inv(kernel).dot(tail)\n",
    "    beta2 = np.linalg.solve(kernel2, tail2)\n",
    "    return beta1, beta2\n",
    "\n",
    "beta1, beta2 = f(0)\n",
    "scatter(beta1, beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
