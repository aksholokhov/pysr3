{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.shields.io/pypi/l/PySR3)\n",
    "![](https://img.shields.io/pypi/v/PySR3)\n",
    "![](https://img.shields.io/github/actions/workflow/status/aksholokhov/pysr3/testing_and_coverage.yml?branch=master)\n",
    "[![](https://img.shields.io/badge/docs-here-green)](https://aksholokhov.github.io/pysr3/)\n",
    "[![codecov](https://codecov.io/gh/aksholokhov/pysr3/branch/master/graph/badge.svg?token=WAA8uIQwjK)](https://codecov.io/gh/aksholokhov/pysr3)\n",
    "[![Codacy Badge](https://app.codacy.com/project/badge/Grade/749695b3c6fd43bb9fdb499ec0ace67b)](https://www.codacy.com/gh/aksholokhov/pysr3/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=aksholokhov/pysr3&amp;utm_campaign=Badge_Grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart with `pysr3`\n",
    "\n",
    "SR3 is a relaxation method designed for accurate feature selection.\n",
    "It currently supports:\n",
    "\n",
    "* Linear Models (LASSO, A-LASSO, CAD, SCAD)\n",
    "* Linear Mixed-Effect Models (L0, LASSO, A-LASSO, CAD, SCAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "pysr3 can be installed via\n",
    "```bash\n",
    " pip install pysr3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Requirements\n",
    "Make sure that Python 3.6 or higher is installed. The package has the following\n",
    "dependencies, as listed in requirements.txt:\n",
    "\n",
    "* numpy>=1.21.1\n",
    "* pandas>=1.3.1\n",
    "* scipy>=1.7.1\n",
    "* PyYAML>=5.4.1\n",
    "* scikit_learn>=0.24.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "pysr3 models are fully compatible to [sklearn standards](https://scikit-learn.org/stable/developers/develop.html),\n",
    "so you can use them as you normally would use a sklearn model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "A simple example of using SR3-empowered LASSO for feature selection is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 300 objects and 500 features; \n",
      "The vector of true parameters contains 55 non-zero elements out of 500.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pysr3.linear.problems import LinearProblem\n",
    "\n",
    "# Create a sample dataset\n",
    "seed = 42\n",
    "num_objects = 300\n",
    "num_features = 500\n",
    "np.random.seed(seed)\n",
    "# create a vector of true model's coefficients\n",
    "true_x = np.random.choice(2, size=num_features, p=np.array([0.9, 0.1]))\n",
    "# create sample data\n",
    "a = 10 * np.random.randn(num_objects, num_features)\n",
    "b = a.dot(true_x) + np.random.randn(num_objects)\n",
    "\n",
    "print(f\"The dataset has {a.shape[0]} objects and {a.shape[1]} features; \\n\"\n",
    "      f\"The vector of true parameters contains {sum(true_x != 0)} non-zero elements out of {num_features}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model found 55 out of 55 features correctly, but also chose 2 extra irrelevant features. \n",
      "The best parameter is {'lam': 0.15055187290939537}\n"
     ]
    }
   ],
   "source": [
    "# Automatic features selection using information criterion\n",
    "from pysr3.linear.models import LinearL1ModelSR3\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "\n",
    "# Here we use SR3-empowered LASSO, but many other popular regularizers are also available\n",
    "# See the glossary of models for more details.\n",
    "model = LinearL1ModelSR3()\n",
    "# We will search for the best model over the range of strengths for the regularizer\n",
    "params = {\n",
    "    \"lam\": loguniform(1e-1, 1e2)\n",
    "}\n",
    "selector = RandomizedSearchCV(estimator=model,\n",
    "                              param_distributions=params,\n",
    "                              n_iter=50,\n",
    "                              # The function below evaluates an information criterion\n",
    "                              # on the test portion of CV-splits.\n",
    "                              scoring=lambda clf, x, y: -clf.get_information_criterion(x, y, ic='bic'))\n",
    "\n",
    "selector.fit(a, b)\n",
    "maybe_x = selector.best_estimator_.coef_['x']\n",
    "tn, fp, fn, tp = confusion_matrix(true_x, maybe_x != 0).ravel()\n",
    "\n",
    "print(f\"The model found {tp} out of {tp + fn} features correctly, but also chose {fp} extra irrelevant features. \\n\"\n",
    "      f\"The best parameter is {selector.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear Mixed-Effects Models\n",
    "\n",
    "Below we show how to use Linear Mixed-Effects (LME) models for simultaneous selection\n",
    "of fixed and random effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pysr3.lme.models import L1LmeModelSR3\n",
    "from pysr3.lme.problems import LMEProblem, LMEStratifiedShuffleSplit\n",
    "\n",
    "problem, true_parameters = LMEProblem.generate(\n",
    "    groups_sizes=[10] * 6,  # 6 groups, 10 objects each\n",
    "    features_labels=[\"fixed+random\"] * 20,  # 20 features, each one having both fixed and random components\n",
    "    beta=np.array([0, 1] * 10),  # True beta (fixed effects) has every other coefficient active\n",
    "    gamma=np.array([0, 0, 0, 1] * 5),  # True gamma (variances of random effects) has every fourth coefficient active\n",
    "    obs_var=0.1  # The errors have standard errors of sqrt(0.1) ~= 0.33\n",
    "\n",
    ")\n",
    "\n",
    "# LMEProblem provides a very convenient representation\n",
    "# of the problem. See the documentation for more details.\n",
    "\n",
    "# It also can be converted to a more familiar representation\n",
    "x, y, columns_labels = problem.to_x_y()\n",
    "# columns_labels describe the roles of the columns in x:\n",
    "# fixed effect, random effect, or both of those, as well as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model found 9 out of 10 correct fixed features, and also chose 2 out of 9 extra irrelevant fixed features. \n",
      "It also identified 5 out of 5 random effects correctly, and got 0 out of 15 non-present random effects. \n",
      "The best sparsity parameter is {'lam': 4.0428727350273315}\n"
     ]
    }
   ],
   "source": [
    "# We use SR3-empowered LASSO model, but many other popular models are also available.\n",
    "# See the glossary of models for more details.\n",
    "model = L1LmeModelSR3()\n",
    "\n",
    "# We're going to select features by varying the strength of the prior\n",
    "# and choosing the model that yields the best information criterion\n",
    "# on the validation set.\n",
    "params = {\n",
    "    \"lam\": loguniform(1e-3, 1e3)\n",
    "}\n",
    "# We use standard functionality of sklearn to perform grid-search.\n",
    "selector = RandomizedSearchCV(estimator=model,\n",
    "                              param_distributions=params,\n",
    "                              n_iter=10,  # number of points from parameters space to sample\n",
    "                              # the class below implements CV-splits for LME models\n",
    "                              cv=LMEStratifiedShuffleSplit(n_splits=2, test_size=0.5,\n",
    "                                                           random_state=seed,\n",
    "                                                           columns_labels=columns_labels),\n",
    "                              # The function below will evaluate the information criterion\n",
    "                              # on the test-sets during cross-validation.\n",
    "                              # We use IC from Muller2018, but other options (AIC, BIC) are also available\n",
    "                              scoring=lambda clf, x, y: -clf.get_information_criterion(x,\n",
    "                                                                                       y,\n",
    "                                                                                       columns_labels=columns_labels,\n",
    "                                                                                       ic=\"muller_ic\"),\n",
    "                              random_state=seed,\n",
    "                              n_jobs=20\n",
    "                              )\n",
    "selector.fit(x, y, columns_labels=columns_labels)\n",
    "best_model = selector.best_estimator_\n",
    "\n",
    "maybe_beta = best_model.coef_[\"beta\"]\n",
    "maybe_gamma = best_model.coef_[\"gamma\"]\n",
    "ftn, ffp, ffn, ftp = confusion_matrix(true_parameters[\"beta\"], abs(maybe_beta) > 1e-2).ravel()\n",
    "rtn, rfp, rfn, rtp = confusion_matrix(true_parameters[\"gamma\"], abs(maybe_gamma) > 1e-2).ravel()\n",
    "\n",
    "print(\n",
    "    f\"The model found {ftp} out of {ftp + ffn} correct fixed features, and also chose {ffp} out of {ftn + ffn} extra irrelevant fixed features. \\n\"\n",
    "    f\"It also identified {rtp} out of {rtp + rfn} random effects correctly, and got {rfp} out of {rtn + rfn} non-present random effects. \\n\"\n",
    "    f\"The best sparsity parameter is {selector.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_skmixed)",
   "language": "python",
   "name": "conda_skmixed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
